{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A self-supervised learning approach using Tensorflow to fill a missing word in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = [\"I like to eat pizza\", \n",
    "             \"She enjoys reading books\",\n",
    "             \"He plays basketball on weekends\",\n",
    "             \"We went to the beach last summer\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sentences to sequences of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pad the sequences to a fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_length = max(len(seq) for seq in sequences)\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model for self-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32)\n",
    "embedding = tf.keras.layers.Embedding(len(tokenizer.word_index)+1, 128)(inputs)\n",
    "context = tf.keras.layers.LSTM(128)(embedding)\n",
    "hidden = tf.keras.layers.Dense(128, activation='relu')(context)\n",
    "output = tf.keras.layers.Dense(len(tokenizer.word_index)+1, activation='softmax')(hidden)\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a masked categorical crossentropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_categorical_crossentropy(y_true, y_pred):\n",
    "    mask = tf.not_equal(y_true, 0)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer='adam', loss=masked_categorical_crossentropy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a dataset for self-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(padded_sequences)\n",
    "dataset = dataset.shuffle(1000).batch(32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for self-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.fit(dataset, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a new sentence with a missing word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence = \"I like to ____ pizza\"\n",
    "sequence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "input_sequence = np.zeros((1, max_length), dtype=np.int32)\n",
    "input_sequence[0, :len(sequence)] = sequence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the missing word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = model.predict(input_sequence)\n",
    "pred_idx = np.argmax(preds[0, :len(sequence)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the predicted index to a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word = tokenizer.index_word[pred_idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the predicted word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Predicted word:\", word)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
